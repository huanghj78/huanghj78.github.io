<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>NLP-MidTermProj IMDB电影评论文本分类</title><meta name="author" content="HogarHuang"><meta name="copyright" content="HogarHuang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="题目要求数据集简介 IMDB数据集包含了50000条电影评论和它们对应的情感极性标签（positive or negative），可以建模为一个文本二分类问题 数据下载地址：https:&#x2F;&#x2F;www.kaggle.com&#x2F;lakshmi25npathi&#x2F;imdb-datasetof-50k-movie-reviews 数据划分：为了方便验收，统一数据集的划分标准。具体为0-30000条为训练集、3">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-MidTermProj IMDB电影评论文本分类">
<meta property="og:url" content="https://huanghj78.github.io/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/index.html">
<meta property="og:site_name" content="Hogar&#39;s Blog">
<meta property="og:description" content="题目要求数据集简介 IMDB数据集包含了50000条电影评论和它们对应的情感极性标签（positive or negative），可以建模为一个文本二分类问题 数据下载地址：https:&#x2F;&#x2F;www.kaggle.com&#x2F;lakshmi25npathi&#x2F;imdb-datasetof-50k-movie-reviews 数据划分：为了方便验收，统一数据集的划分标准。具体为0-30000条为训练集、3">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://huanghj78.github.io/images/background.jpg">
<meta property="article:published_time" content="2022-07-30T08:51:40.222Z">
<meta property="article:modified_time" content="2022-07-30T09:20:46.600Z">
<meta property="article:author" content="HogarHuang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://huanghj78.github.io/images/background.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://huanghj78.github.io/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":50,"languages":{"author":"Author: HogarHuang","link":"Link: ","source":"Source: Hogar's Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://fastly.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP-MidTermProj IMDB电影评论文本分类',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-30 17:20:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="is-center"><div class="avatar-img"><img src="/img/luka.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HogarHuang</div><div class="author-info__description">1353188493@qq.com</div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/records"><i class="fa-fw fas fa-comment-dots"></i><span> 随记</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/game"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Hogar's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/records"><i class="fa-fw fas fa-comment-dots"></i><span> 随记</span></a></div><div class="menus_item"><a class="site-page" href="/gedan"><i class="fa-fw fas fa-music"></i><span> 歌单</span></a></div><div class="menus_item"><a class="site-page" href="/game"><i class="fa-fw fas fa-gamepad"></i><span> 游戏</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk"><i class="fa-fw fa fa-heartbeat"></i><span> 时光</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/google"><span> 镜像</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://json.xbyzs.cf"><span> Json格式化</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://draw.xbyzs.cf"><span> Draw画布</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://enkey.xbyzs.cf"><span> EnKey</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">NLP-MidTermProj IMDB电影评论文本分类</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2022-07-30T08:51:40.222Z" title="Created 2022-07-30 16:51:40">2022-07-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><h1 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h1><h2 id="数据集简介"><a href="#数据集简介" class="headerlink" title="数据集简介"></a>数据集简介</h2><ol>
<li>IMDB数据集包含了50000条电影评论和它们对应的情感极性标签<br>（positive or negative），可以建模为一个文本二分类问题</li>
<li>数据下载地址：<a target="_blank" rel="noopener" href="https://www.kaggle.com/lakshmi25npathi/imdb-datasetof-50k-movie-reviews">https://www.kaggle.com/lakshmi25npathi/imdb-datasetof-50k-movie-reviews</a></li>
<li>数据划分：为了方便验收，统一数据集的划分标准。具体为0-30000条<br>为训练集、30001-40000条为验证集和40001-50000条为测试集。模型<br>的性能以测试集的结果为最终标准<h2 id="数据清洗和文本预处理"><a href="#数据清洗和文本预处理" class="headerlink" title="数据清洗和文本预处理"></a>数据清洗和文本预处理</h2></li>
<li>原始数据是爬虫得到的初步结果，里面包含了HTML标签和URL等与文<br>本情感无关的噪音，需要进行数据清洗</li>
<li>自行决定是否要统一单词大小写、去除停用词和低频词以及标点符号等<h2 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h2></li>
<li>特征：word2vec特征</li>
<li>分类方法：对比不同的循环神经网络(RNN、LSTM、GRU)</li>
<li>不允许使用现成的线上情感分析平台（API）</li>
</ol>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> SnowballStemmer</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding, LSTM, GRU, Dropout, Dense, Input</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">RAW_DATA = <span class="string">&quot;IMDB Dataset.csv&quot;</span>  <span class="comment"># 源数据</span></span><br><span class="line">COP = re.<span class="built_in">compile</span>(<span class="string">&quot;[^\u4e00-\u9fa5^a-z^A-Z^\x20]&quot;</span>)  <span class="comment"># 只保留中文、字母、数字、空格</span></span><br><span class="line">STOP_WORDS = <span class="built_in">set</span>(stopwords.words(<span class="string">&#x27;english&#x27;</span>))  <span class="comment"># 导入停用词表</span></span><br><span class="line">RNN_MODEL = <span class="string">&quot;RNN.model&quot;</span>  <span class="comment"># 模型名称</span></span><br><span class="line">MAX_LEN = <span class="number">1000</span>  <span class="comment"># 每组数据统一的长度</span></span><br><span class="line">DIC_SIZE = <span class="number">10000</span>  <span class="comment"># 单词字典大小</span></span><br><span class="line">VEC_SIZE = <span class="number">256</span>  <span class="comment"># 数据向量化维度</span></span><br><span class="line">OUTPUT_DIM = <span class="number">128</span>  <span class="comment"># RNN层输出维度</span></span><br><span class="line"></span><br><span class="line">stemmer = SnowballStemmer(<span class="string">&quot;english&quot;</span>)  <span class="comment"># 用于词干化</span></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&quot;&lt;[^&gt;]+&gt;&quot;</span>, re.S)  <span class="comment"># 用于消除HTML标签</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    df = pd.read_csv(RAW_DATA)</span><br><span class="line">    review = df[<span class="string">&#x27;review&#x27;</span>]</span><br><span class="line">    sentiment = df[<span class="string">&#x27;sentiment&#x27;</span>]</span><br><span class="line">    reviews = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> review:</span><br><span class="line">        <span class="comment"># 去除HTML标签</span></span><br><span class="line">        row = pattern.sub(<span class="string">&#x27;&#x27;</span>, row)</span><br><span class="line">        <span class="comment"># 去除特殊符号</span></span><br><span class="line">        row = re.sub(COP, <span class="string">&#x27;&#x27;</span>, row)</span><br><span class="line">        <span class="comment"># 去停用词并词干化</span></span><br><span class="line">        words = [stemmer.stem(w) <span class="keyword">for</span> w <span class="keyword">in</span> row.strip(</span><br><span class="line">        ).lower().split() <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> STOP_WORDS]</span><br><span class="line">        row = <span class="string">&quot; &quot;</span>.join(words)</span><br><span class="line">        reviews.append(row)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 文本序列化，建立一个单词量为DIC_SIZE的字典并将数据中的单词转化为对应的数字</span></span><br><span class="line">    token = Tokenizer(num_words=DIC_SIZE)</span><br><span class="line">    token.fit_on_texts(reviews)</span><br><span class="line">    x_data = token.texts_to_sequences(reviews)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据标签数值化，积极为1，消极为0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(df[<span class="string">&#x27;sentiment&#x27;</span>])):</span><br><span class="line">        <span class="keyword">if</span> df[<span class="string">&#x27;sentiment&#x27;</span>][i] == <span class="string">&#x27;positive&#x27;</span>:</span><br><span class="line">            df[<span class="string">&#x27;sentiment&#x27;</span>][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[<span class="string">&#x27;sentiment&#x27;</span>][i] = <span class="number">0</span></span><br><span class="line">    sentiment_data = df[<span class="string">&#x27;sentiment&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据标准化</span></span><br><span class="line">    y_data = np.array(sentiment_data).astype(np.float32)</span><br><span class="line">    x_data = sequence.pad_sequences(x_data, maxlen=MAX_LEN).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据集划分</span></span><br><span class="line">    x_train = x_data[:<span class="number">30000</span>]</span><br><span class="line">    y_train = y_data[:<span class="number">30000</span>]</span><br><span class="line">    x_test = x_data[<span class="number">30000</span>:<span class="number">40000</span>]</span><br><span class="line">    y_test = y_data[<span class="number">30000</span>:<span class="number">40000</span>]</span><br><span class="line">    x_valid = x_data[<span class="number">40000</span>:]</span><br><span class="line">    y_valid = y_data[<span class="number">40000</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Embedding(</span><br><span class="line">        output_dim=VEC_SIZE,</span><br><span class="line">        input_dim=DIC_SIZE,</span><br><span class="line">        input_length=MAX_LEN</span><br><span class="line">    ))</span><br><span class="line">    model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">    model.add(keras.layers.SimpleRNN(OUTPUT_DIM))</span><br><span class="line">    <span class="comment"># model.add(keras.layers.LSTM(OUTPUT_DIM))</span></span><br><span class="line">    <span class="comment"># model.add(keras.layers.GRU(OUTPUT_DIM))</span></span><br><span class="line">    model.add(Dense(units=<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">    model.add(Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">    model.summary()</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    model.fit(x_train, y_train, batch_size=<span class="number">256</span>,</span><br><span class="line">              epochs=<span class="number">10</span>, verbose=<span class="number">2</span>,</span><br><span class="line">              validation_data=(x_valid, y_valid))</span><br><span class="line">    <span class="comment"># save checkpoint</span></span><br><span class="line">    model.save(RNN_MODEL)</span><br><span class="line">    <span class="comment"># 模型测试</span></span><br><span class="line">    loss, accuracy = model.evaluate(x_test, y_test, batch_size=<span class="number">256</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(accuracy))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="实验报告"><a href="#实验报告" class="headerlink" title="实验报告"></a>实验报告</h1><h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><p>本实验是利用自然语言处理模型对IMDB电影评论文本进行情感分类，IMDB数据集包含了50000条电影评论和它们对应的情感极性标签（positive or negative），可以建模为一个文本二分类问题</p>
<h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><ul>
<li>熟悉和掌握基本的文本清洗步骤和实践</li>
<li>熟悉文本的表示方法</li>
<li>熟悉将常用的深度学习模型应用于文本分类问题的流程和范式</li>
</ul>
<h2 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h2><h3 id="1-数据下载"><a href="#1-数据下载" class="headerlink" title="1. 数据下载"></a>1. 数据下载</h3><p>根据作业要求到指定网站<code>https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews</code> 下载源数据解压放到工作目录下，命名为<code>IMDB Dataset.csv</code></p>
<h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><p>获得的原始数据是网页爬虫的结果，因此首先需要去除其中的HTML标签等与文本情感无关的噪音。接着再对数据做进一步的处理，包括去除特殊符号、分词、去停用词并词干化等，实现的过程如下：</p>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529135939656.png" alt="image-20220529135939656"></p>
<p>比较预处理结果，以第一条数据为例</p>
<p>原始数据：</p>
<blockquote>
<p>One of the other reviewers has mentioned that after watching just 1 Oz episode you’ll be hooked. They are right, as this is exactly what happened with me.<br><br>The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br><br>It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more….so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br><br>I would say the main appeal of the show is due to the fact that it goes where other shows wouldn’t dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance…OZ doesn’t mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn’t say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who’ll be sold out for a nickel, inmates who’ll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing….thats if you can get in touch with your darker side.</p>
</blockquote>
<p>处理后：</p>
<blockquote>
<p>one review mention watch oz episod youll hook right exact happen meth first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus main emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti surreal couldnt say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side</p>
</blockquote>
<p>可以看到HTML标签、停用词如of、the等已被删去，reviewers、mentioned被词干化为review、mention，由此使得数据更简洁凝练，减小了后续序列化的代价</p>
<h3 id="3-特征提取"><a href="#3-特征提取" class="headerlink" title="3. 特征提取"></a>3. 特征提取</h3><p>此次模型的搭建是利用keras库的sequential完成，我们可以借助第一层Embedding层来进行文本的向量化，而根据该层的要求，首先需要将输入的文本序列化，可以借助Tokenizer完成。这里建立一个大小为DIC_SIZE的字典，即保留出现频率最高的前DIC_SIZE个单词，为其编上序号</p>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529141423809.png" alt="image-20220529141423809"></p>
<p>对结果进行验证：</p>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529143311900.png" alt="image-20220529143311900"></p>
<blockquote>
<p>one review mention watch oz episod youll hook right exact happen meth first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus main emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti surreal couldnt say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side won</p>
<p>[3, 240, 363, 11, 2800, 198, 413, 1575, 116, 463, 112, 5199, 30, 34, 2758, 2800, 1080, 9467, 16, 542, 94, 116, 302, 26, 1352, 17, 4405, 412, 7715, 17, 581, 1674, 1067, 660, 334, 542, 2899, 246, 64, 172, 2800, 5965, 309, 5315, 1798, 527, 577, 174, 451, 3318, 1888, 781, 1891, 1773, 870, 244, 149, 3693, 3094, 451, 292, 3410, 9182, 5697, 1062, 918, 2122, 248, 1996, 5698, 422, 6521, 6208, 49, 160, 14, 39, 174, 792, 17, 605, 102, 202, 17, 498, 1443, 684, 106, 289, 1038, 2157, 189, 684, 615, 684, 74, 751, 109, 30, 198, 55, 131, 2758, 1349, 1707, 355, 39, 1396, 11, 375, 927, 2800, 114, 7480, 149, 489, 1179, 542, 542, 5101, 2888, 1714, 2536, 4280, 103, 474, 8, 177, 20, 1018, 700, 653, 4280, 98, 781, 4017, 605, 297, 626, 989, 781, 358, 11, 2800, 121, 136, 1759, 2562, 8, 459, 3319, 393] </p>
<p>3 </p>
<p>240</p>
</blockquote>
<p>可以看到，单词one对应的序号为3，单词review对应的序号为240，这与输出第一条数据的序列化结果是一致的</p>
<p>同样的，也对标签进行数值化</p>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529143445692.png" alt="image-20220529143445692"></p>
<h3 id="4-数据标准化及划分"><a href="#4-数据标准化及划分" class="headerlink" title="4. 数据标准化及划分"></a>4. 数据标准化及划分</h3><p>将数据转化为numpy类型以便传入模型，同时将每组数据进行padding实现等长，最后再根据题目要求，0-30000条为训练集、30001-40000条为验证集和40001-50000条为测试集</p>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220530154340636.png" alt="image-20220530154340636"></p>
<h3 id="5-搭建模型"><a href="#5-搭建模型" class="headerlink" title="5. 搭建模型"></a>5. 搭建模型</h3><h4 id><a href="#" class="headerlink" title></a><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220530194457781.png" alt="image-20220530194457781"></h4><p>第一层为Embedding层，用于对输入的序列化的数据进行向量化</p>
<ul>
<li>输出维度，即每组数据向量化之后的向量大小为VEC_SIZE</li>
<li>输入维度即为前面由Tokenizer生成的字典大小</li>
<li>输入长度即为每组数据序列化之后的长度，这也是为什么要进行统一长度的原因</li>
</ul>
<p>紧接着连接一个Dropout层，以0.25的的机率随机丢弃输入以防止过拟合</p>
<p>接下来就可以连接一个RNN层或是LSTM或GRU</p>
<p>最后将输出经过两层全连接层进行分类得到最终结果，第一个全连接层选择Relu作为激活函数，而由于此问题为二分类问题，故最终的输出层选择sigmoid作为激活函数</p>
<p>损失函数选择二元交叉熵函数，优化算法使用Adam优化器</p>
<p>这里SimpleRNN、LSTM和GRU层大部分参数都选择了默认参数，它们的数值如下，以SimpleRNN为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.SimpleRNN(</span><br><span class="line">    units,</span><br><span class="line">    activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">    bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>,</span><br><span class="line">    recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">    return_sequences=<span class="literal">False</span>,</span><br><span class="line">    return_state=<span class="literal">False</span>,</span><br><span class="line">    go_backwards=<span class="literal">False</span>,</span><br><span class="line">    stateful=<span class="literal">False</span>,</span><br><span class="line">    unroll=<span class="literal">False</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>注释展示了三种不同的模型，可以通过修改注释来比较不同模型的效果</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN的结构是由一个输入层、隐藏层、输出层组成：</p>
<p><img src="https://pic1.zhimg.com/v2-b8a6c264b2275569550ef1d88b09da4c_r.jpg" alt="preview" style="zoom:50%;"></p>
<p>内部的运算过程为，(t-1)时刻的隐层输出与w矩阵相乘，与t时刻的输入乘以u之后的值进行相加，然后经过一个非线性变化（tanh或Relu），然后以此方式传递给下一个时刻。这样容易出现梯度消失的问题，于是可以通过LSTM模型来解决这一问题。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>LSTM，即长短期记忆网络，是一种特殊的RNN类型，其优势在于可以从语料中学习到长期依赖关系。LSTM主要由遗忘门、输入门、输出门与记忆单元组成。如下图所示</p>
<p><img src="https://img-blog.csdnimg.cn/65c4f3a88ff3404e9a81ce7bd7fdd36d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5pyA5ZCO55qE54KJ6L65,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom:67%;"></p>
<p>遗忘门：读取h<sub>t-1</sub> 和x<sub>t</sub>，经过sigmoid，输入一个在0到1之间数值给每个在记忆单元c<sub>t-1</sub>中的数字，1表示完全保留，0表示完全舍弃。</p>
<p>输入门：确定什么样的信息内存放在记忆单元中，包含两部分：</p>
<p>​                <img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529172613804.png" alt="image-20220529172613804"></p>
<p>记忆单元：更新方式为：旧状态c<sub>t-1</sub>与f<sub>t</sub>相乘，遗忘掉由f<sub>t</sub>所确定的需要遗忘的信息，再加上i<sub>t</sub>*c<sub>t</sub>，得到新的记忆单元</p>
<p>输出门：首先sigmoid层确定记忆单元的那些信息被传递出去，然后把细胞状态通过 tanh层 进行处理（得到[-1,1]的值)并将它和输出门的输出相乘，最终外部状态仅仅会得到输出门确定输出的那部分。</p>
<p>可以看到，LSTM通过增加计算单元的复杂度以解决因长期依赖带来的梯度消失和梯度爆炸问题，但显然，LSTM的参数是明显增多的，因此也增加了训练难度。于是可以再看另外一种模型——GRU</p>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>GRU是LSTM的一种变体，它相比LSTM少了一个门控结构，将 LSTM 中的输入门和遗忘门合二为一，称为更新门。另外，GRU取消进行线性自更新的记忆单元，而是直接在隐藏单元中利用门控直接进行线性自更新。其结构如下</p>
<p><img src="https://img-blog.csdnimg.cn/20210428201614728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXNpc2lf,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;"></p>
<p>GRU由重置门和更新门组成，简单地来说重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。</p>
<h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>此次实验比对了三种神经网络模型——RNN、LSTM和GRU</p>
<h3 id="RNN-1"><a href="#RNN-1" class="headerlink" title="RNN"></a>RNN</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 10</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220530162622704.png" alt="image-20220530162622704"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 32</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529180338101.png" alt="image-20220529180338101"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 128</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220530171425994.png" alt="image-20220530171425994"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 256</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220530191959856.png" alt="image-20220530191959856"></p>
<p>我们通过改变VEC_SIZE可以看到，当向量化后的维度过小时，效果不是很好，当维度只有10时，准确度只有0.77左右，而随着维度增大到32，准确度有所提高，来到了0.86，然而继续提高到128甚至到256，效果并没有显著变化了，反倒耗时变长了。</p>
<h3 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 32</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529190225850.png" alt="image-20220529190225850"></p>
<p>对比RNN层可以看到，LSTM层的参数数量为RNN参数的四倍，训练的时长也相应地变长，不过从最终的结果来看两者并为显示出明显的差别。</p>
<h3 id="GRU-1"><a href="#GRU-1" class="headerlink" title="GRU"></a>GRU</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = 256</span><br><span class="line">DIC_SIZE = 10000</span><br><span class="line">VEC_SIZE = 32</span><br><span class="line">OUTPUT_DIM = 64</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/image-20220529192251881.png" alt="image-20220529192251881"></p>
<p>对比LSTM层，GRU层的参数数量约为LSTM的四分之三，训练时长也相应地减短，而结果仍未有明显差别。</p>
<p>经由三个模型的对比，可以感受到它们耗时的不同，不过对于此问题，三者的准确度并未有明显差别，原因可能是在于LSTM、GRU模型相较RNN的优势在于对存在长依赖的文本有更好的判别效果，比如推测单词、补全句子的情形。但是我们现解决的是文本的二分类问题，更重要的是靠文本中一些关键词来进行分类，并不会特别依赖于长依赖的句子，因此便体现不出它们的优势，反而会因为参数过多而徒增训练时间。</p>
<h2 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h2><p>​        经过此次作业，自己亲手体验了将深度学习模型应用于文本分类问题的过程，在此过程中，我学会了如何进行数据清洗，了解了文本去停用词、词干化的意义。由于整个模型的建立是借助keras库，因此我也尝试了除之前作业所用到的jieba分词，word2vec向量化以外的方法来对文本进行特征提取，这让我对于课上所学的知识有了更深的理解。在模型搭建的过程中，也通过网上搜索资料并结合课上所学知识，进一步理解深度学习模型的原理，对于Embedding层，Dropout层乃至损失函数、激活函数有了切实的使用体验，也由此更了解它们的工作原理。通过比对三种常见的循环神经网络模型，我对于循环神经网络的结构、原理有了进一步理解，通过实际尝试体验了它们彼此之间的一些优劣之处，进一步体会到了所谓没有最好的模型，只有最合适的模型。</p>
<p>​        总之，经过本次作业，我基本掌握了如何使用深度学习模型进行文本分类，学会了一些基础的实验流程和范式，也对循环神经网络模型有了进一步的知识巩固和扩展。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">HogarHuang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://huanghj78.github.io/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/">https://huanghj78.github.io/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-MidTermProj/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post_share"><div class="social-share" data-image="/images/background.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://fastly.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/07/30/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/ST-Assignment1/"><img class="prev-cover" src="/images/background.jpg" onerror="onerror=null;src='/img/night.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">软件测试-Assignment1 软件生命周期</div></div></a></div><div class="next-post pull-right"><a href="/2022/07/30/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/NLP-Exercise2/"><img class="next-cover" src="/images/background.jpg" onerror="onerror=null;src='/img/night.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">NLP-Exercise2  Word2Vec</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%98%E7%9B%AE%E8%A6%81%E6%B1%82"><span class="toc-number">1.</span> <span class="toc-text">题目要求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">数据集简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E5%92%8C%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据清洗和文本预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">分类模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A"><span class="toc-number">3.</span> <span class="toc-text">实验报告</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9"><span class="toc-number">3.1.</span> <span class="toc-text">实验内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-number">3.2.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">实验方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD"><span class="toc-number">3.3.1.</span> <span class="toc-text">1. 数据下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.3.2.</span> <span class="toc-text">2. 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">3.3.3.</span> <span class="toc-text">3. 特征提取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%8F%8A%E5%88%92%E5%88%86"><span class="toc-number">3.3.4.</span> <span class="toc-text">4. 数据标准化及划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%90%AD%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.5.</span> <span class="toc-text">5. 搭建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link"><span class="toc-number">3.3.5.1.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN"><span class="toc-number">3.3.5.2.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LSTM"><span class="toc-number">3.3.5.3.</span> <span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GRU"><span class="toc-number">3.3.5.4.</span> <span class="toc-text">GRU</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">3.4.</span> <span class="toc-text">结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-1"><span class="toc-number">3.4.1.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM-1"><span class="toc-number">3.4.2.</span> <span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU-1"><span class="toc-number">3.4.3.</span> <span class="toc-text">GRU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A"><span class="toc-number">3.5.</span> <span class="toc-text">心得体会</span></a></li></ol></li></ol></div></div><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/luka.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HogarHuang</div><div class="author-info__description">1353188493@qq.com</div></div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2022 By HogarHuang</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/chenxz21/hexo-theme-bcxm">Bcxm</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://fastly.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script type="text/javascript" src="https://fastly.jsdelivr.net/npm/leancloud-storage@4.10.0/dist/av-min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://fastly.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>